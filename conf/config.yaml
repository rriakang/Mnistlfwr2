# Common
random_seed: 42

learning_rate: 0.001 # Input model's learning rate

# model_type: 'Pytorch'
 # This value should be maintained
model_type: 'hyperparameter' # This value should be maintained
model:
  _target_: models.MNISTClassifier # Input your custom model
  output_size: 10 # Input your model's output size (only classification)

dataset:
    name: 'MNIST' # Input your data name
    validation_split: 0.2 # Ratio of dividing train data by validation


# client
task_id: 'mnist2' # Input your Task Name that you register in FedOps Website

wandb: 
  use: true # Whether to use wandb
  key: '38f6cf3c2c37660d42ebfe8ab434b72d34be3b31' # Input your wandb api key
  account: 'rirakang@gachon.ac.kr' # Input your wandb account
  project: '${dataset.name}_${task_id}'


# server
num_epochs: 1 # number of local epochs
batch_size: 128
num_rounds: 1 # Number of rounds to perform
clients_per_round: 1 # Number of clients participating in the round


server:
  strategy:
    _target_: flwr.server.strategy.FedAvg # aggregation algorithm (defalut: fedavg)
    fraction_fit: 1.0 # because we want the number of clients to sample on each round to be solely defined by min_fit_clients
    fraction_evaluate: 0.000001 # because we want the number of clients to sample on each round to be solely defined by min_fit_clients
    min_fit_clients: ${clients_per_round} # Minimum number of clients to participate in training
    min_available_clients: ${clients_per_round} # Minimum number of clients to participate in a round
    min_evaluate_clients: ${clients_per_round} # Minimum number of clients to participate in evaluation
    on_fit_config_fn:
      _target_: fedops.server_config.get_on_fit_config
      learning_rate: ${learning_rate} 
      batch_size: ${batch_size}
      local_epochs: ${num_epochs}
      num_rounds: ${num_rounds}
